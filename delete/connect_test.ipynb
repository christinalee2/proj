{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65ea6f30-be6c-4cf7-b199-8dcbd8407961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.0\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import awswrangler as wr\n",
    "print(wr.__version__)\n",
    "print(hasattr(wr, \"athena\"))\n",
    "print(hasattr(wr.athena, \"read_sql_query\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f29e779-e704-43da-bcd5-b78fbd84edce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING ID COLUMNS IN TABLES\n",
      "============================================================\n",
      "\n",
      "Table: institution\n",
      "------------------------------\n",
      "  Total columns: 12\n",
      "  ID-like columns: ['id_institution']\n",
      "    id_institution:\n",
      "      All null values\n",
      "  All columns: ['id_institution', 'last_verified', 'institution_cpi', 'institution_cpi_short', 'institution_type_layer1', 'institution_type_layer2', 'institution_type_layer3', 'double_counting_risk', 'country_sub', 'country_parent', 'contact_info', 'comments']\n",
      "\n",
      "Table: geography\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 12:49:15.197 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:15.419 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/homebrew/Caskroom/miniforge/base/envs/cpi-data/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-16 12:49:15.419 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:15.420 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:15.422 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:15.422 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:15.423 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No data found\n",
      "\n",
      "Table: sector\n",
      "------------------------------\n",
      "  Total columns: 14\n",
      "  ID-like columns: ['sector_key']\n",
      "    sector_key:\n",
      "      Sample values: ['AF_AG_PR', 'AF_AG_SC', 'AF_AG_FS', 'AF_AG_MI', 'AF_AG_AD']\n",
      "      Non-numeric values\n",
      "  All columns: ['sector_key', 'sector', 'c1', 'sub_sector', 'c2', 'solution', 'c3', 're', 'ff', 'ee', 'og', 'lt', 'mi', 'ad']\n",
      "\n",
      "Table: instrument\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 12:49:18.428 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:18.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:18.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:18.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:18.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:18.433 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No data found\n",
      "\n",
      "Table: gender\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 12:49:19.825 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:19.838 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:19.839 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:19.841 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:19.842 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:19.842 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No data found\n",
      "\n",
      "Table: data_source\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 12:49:21.258 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:21.259 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:21.260 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:21.261 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:21.261 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:21.262 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No data found\n",
      "\n",
      "Table: recipient\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 12:49:22.684 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:22.686 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:22.687 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:22.691 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:22.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:22.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No data found\n",
      "\n",
      "============================================================\n",
      "TESTING ID DETECTION ALGORITHM\n",
      "============================================================\n",
      "\n",
      "Table: institution\n",
      "Error testing institution: cannot import name 'IDManager' from 'database.connection' (/Users/christinalee/repos/ref-frontend/database/connection.py)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 12:49:25.690 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:25.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:25.693 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:25.696 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:25.697 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-16 12:49:25.698 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table: sector\n",
      "Error testing sector: cannot import name 'IDManager' from 'database.connection' (/Users/christinalee/repos/ref-frontend/database/connection.py)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to detect ID columns in your tables\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('.')\n",
    "\n",
    "from database.connection import DatabaseConnection\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def analyze_table_columns():\n",
    "    \"\"\"Analyze all tables to find ID column patterns\"\"\"\n",
    "    \n",
    "    tables_to_check = ['institution', 'geography', 'sector', 'instrument', 'gender', 'data_source', 'recipient']\n",
    "    \n",
    "    print(\"ANALYZING ID COLUMNS IN TABLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for table_name in tables_to_check:\n",
    "        print(f\"\\nTable: {table_name}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            # Get sample data\n",
    "            df = DatabaseConnection.get_table_data(table_name, limit=5)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"  No data found\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Total columns: {len(df.columns)}\")\n",
    "            \n",
    "            # Look for ID-like columns\n",
    "            id_columns = []\n",
    "            for col in df.columns:\n",
    "                col_lower = col.lower()\n",
    "                if any(pattern in col_lower for pattern in ['id', 'key']):\n",
    "                    id_columns.append(col)\n",
    "            \n",
    "            print(f\"  ID-like columns: {id_columns}\")\n",
    "            \n",
    "            # For each ID column, analyze the data\n",
    "            for id_col in id_columns:\n",
    "                print(f\"    {id_col}:\")\n",
    "                \n",
    "                # Check data type and sample values\n",
    "                non_null_values = df[id_col].dropna()\n",
    "                if len(non_null_values) > 0:\n",
    "                    print(f\"      Sample values: {list(non_null_values.head())}\")\n",
    "                    \n",
    "                    # Check if numeric\n",
    "                    try:\n",
    "                        numeric_values = pd.to_numeric(non_null_values, errors='coerce')\n",
    "                        if not numeric_values.isna().all():\n",
    "                            print(f\"      Min: {numeric_values.min()}, Max: {numeric_values.max()}\")\n",
    "                            print(f\"      Unique values: {len(numeric_values.unique())}/{len(numeric_values)}\")\n",
    "                        else:\n",
    "                            print(\"      Non-numeric values\")\n",
    "                    except:\n",
    "                        print(\"      Could not analyze as numeric\")\n",
    "                else:\n",
    "                    print(\"      All null values\")\n",
    "            \n",
    "            # Show all columns for reference\n",
    "            print(f\"  All columns: {list(df.columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "\n",
    "def test_id_detection_algorithm():\n",
    "    \"\"\"Test the ID detection algorithm on actual data\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TESTING ID DETECTION ALGORITHM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    tables = ['institution', 'geography', 'sector']\n",
    "    \n",
    "    for table_name in tables:\n",
    "        try:\n",
    "            df = DatabaseConnection.get_table_data(table_name, limit=10)\n",
    "            \n",
    "            if df.empty:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nTable: {table_name}\")\n",
    "            \n",
    "            # Test our ID detection logic\n",
    "            from database.connection import IDManager\n",
    "            \n",
    "            detected_id = IDManager.find_id_column(df, table_name)\n",
    "            print(f\"  Detected ID column: {detected_id}\")\n",
    "            \n",
    "            if detected_id:\n",
    "                next_id = IDManager.get_next_id(df, detected_id)\n",
    "                print(f\"  Next ID would be: {next_id}\")\n",
    "                \n",
    "                # Validate the column\n",
    "                is_valid, issues = IDManager.validate_ids(df, table_name)\n",
    "                print(f\"  Validation: {'PASS' if is_valid else 'FAIL'}\")\n",
    "                if issues:\n",
    "                    print(f\"  Issues: {issues}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error testing {table_name}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_table_columns()\n",
    "    test_id_detection_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207f42b1-cb34-4a19-90c3-ef10d6d0ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING SINGLE-FILE TABLE INSERTION\n",
      "============================================================\n",
      "Checking table structure in S3...\n",
      "Institution table file exists: s3://cpi-uk-us-datascience-stage/auxiliary-data/reference-data/reference-db/institution/data.parquet\n",
      "  Size: 893986 bytes\n",
      "  Last modified: 2025-10-07 18:25:31+00:00\n",
      "\n",
      "Reading current institution data...\n",
      "Current row count: 5\n",
      "Sample records:\n",
      "         institution_cpi institution_type_layer1              country_sub\n",
      "100% RE IPP GmbH & Co KG                 Private                  Germany\n",
      "           123Venture SA                 Private                   France\n",
      "          127 Energy LLC                 Private United States of America\n",
      "   174 Power Global Corp                 Private United States of America\n",
      "         1st Source Bank                 Private United States of America\n",
      "\n",
      "==================================================\n",
      "TESTING SINGLE INSERT\n",
      "==================================================\n",
      "Inserting: Test Institution 20251016_121436\n",
      "Rows before insert: 31275\n",
      "Starting insert for table: institution\n",
      "Reading existing data from s3://cpi-uk-us-datascience-stage/auxiliary-data/reference-data/reference-db/institution/data.parquet\n",
      "Found 31275 existing rows\n",
      "Adding to existing 31275 rows\n",
      "Final dataset will have 31276 rows\n",
      "Writing 31276 rows to s3://cpi-uk-us-datascience-stage/auxiliary-data/reference-data/reference-db/institution/data.parquet\n",
      "Created backup: s3://cpi-uk-us-datascience-stage/auxiliary-data/reference-data/reference-db/institution/data.parquet.backup.20251016_121450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 12:14:54.789 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated: s3://cpi-uk-us-datascience-stage/auxiliary-data/reference-data/reference-db/institution/data.parquet\n",
      "Successfully added 1 row to institution table\n",
      "Cleared Streamlit cache\n",
      "Insert operation completed successfully\n",
      "Waiting 3 seconds for consistency...\n",
      "Rows after insert: 62551\n",
      "Success! Added 31276 row(s)\n",
      "Record found in table:\n",
      "id_institution  last_verified                  institution_cpi institution_cpi_short institution_type_layer1 institution_type_layer2 institution_type_layer3 double_counting_risk   country_sub country_parent contact_info comments\n",
      "          None           2025 Test Institution 20251016_121436                  None                 Private             Corporation               Corporate                 None United States  United States         None     None\n",
      "\n",
      "==================================================\n",
      "TESTING BULK INSERT\n",
      "==================================================\n",
      "Inserting 2 records\n",
      "Rows before bulk insert: 62551\n",
      "Starting bulk insert of 2 rows for table: institution\n",
      "Reading existing data from s3://cpi-uk-us-datascience-stage/auxiliary-data/reference-data/reference-db/institution/data.parquet\n",
      "Found 31276 existing rows\n",
      "Adding to existing 31276 rows\n",
      "Final dataset will have 31278 rows\n",
      "Writing 31278 rows to s3://cpi-uk-us-datascience-stage/auxiliary-data/reference-data/reference-db/institution/data.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christinalee/repos/ref-frontend/database/connection.py:268: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  updated_df = pd.concat([existing_df, new_rows_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created backup: s3://cpi-uk-us-datascience-stage/auxiliary-data/reference-data/reference-db/institution/data.parquet.backup.20251016_121541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 12:15:43.484 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated: s3://cpi-uk-us-datascience-stage/auxiliary-data/reference-data/reference-db/institution/data.parquet\n",
      "Successfully added 2 rows to institution table\n",
      "Cleared Streamlit cache\n",
      "Bulk insert operation completed successfully\n",
      "Waiting 3 seconds for consistency...\n",
      "Rows after bulk insert: 93829\n",
      "Success! Added 31278 row(s)\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "🎉 ALL TESTS PASSED!\n",
      "Your insert logic is now working correctly.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test script for the fixed single-file table insertion logic\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('.')\n",
    "\n",
    "from database.connection import DatabaseConnection\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "def check_table_structure():\n",
    "    \"\"\"Check the actual table structure in S3\"\"\"\n",
    "    print(\"Checking table structure in S3...\")\n",
    "    \n",
    "    s3_client = boto3.client('s3', region_name='us-east-1')\n",
    "    bucket = 'cpi-uk-us-datascience-stage'\n",
    "    \n",
    "    # Check institution table file\n",
    "    institution_key = 'auxiliary-data/reference-data/reference-db/institution/data.parquet'\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.head_object(Bucket=bucket, Key=institution_key)\n",
    "        print(f\"Institution table file exists: s3://{bucket}/{institution_key}\")\n",
    "        print(f\"  Size: {response['ContentLength']} bytes\")\n",
    "        print(f\"  Last modified: {response['LastModified']}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Institution table file not found: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_current_data():\n",
    "    \"\"\"Read current data from the institution table\"\"\"\n",
    "    print(\"\\nReading current institution data...\")\n",
    "    \n",
    "    try:\n",
    "        df = DatabaseConnection.get_table_data('institution', limit=5)\n",
    "        print(f\"Current row count: {len(df) if not df.empty else 0}\")\n",
    "        \n",
    "        if not df.empty:\n",
    "            print(\"Sample records:\")\n",
    "            print(df[['institution_cpi', 'institution_type_layer1', 'country_sub']].to_string(index=False))\n",
    "        else:\n",
    "            print(\"No data found\")\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_single_insert():\n",
    "    \"\"\"Test inserting a single record using the new method\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING SINGLE INSERT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create unique test data\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    test_data = {\n",
    "        'institution_cpi': f'Test Institution {timestamp}',\n",
    "        'institution_type_layer1': 'Private',\n",
    "        'institution_type_layer2': 'Corporation',\n",
    "        'institution_type_layer3': 'Corporate',\n",
    "        'country_sub': 'United States',\n",
    "        'country_parent': 'United States',\n",
    "        'last_verified': datetime.now().year\n",
    "    }\n",
    "    \n",
    "    print(f\"Inserting: {test_data['institution_cpi']}\")\n",
    "    \n",
    "    # Get row count before\n",
    "    before_df = DatabaseConnection.get_table_data('institution')\n",
    "    before_count = len(before_df) if not before_df.empty else 0\n",
    "    print(f\"Rows before insert: {before_count}\")\n",
    "    \n",
    "    # Attempt insert\n",
    "    success = DatabaseConnection.execute_insert('institution', test_data)\n",
    "    \n",
    "    if success:\n",
    "        print(\"Insert operation completed successfully\")\n",
    "        \n",
    "        # Check row count after\n",
    "        print(\"Waiting 3 seconds for consistency...\")\n",
    "        import time\n",
    "        time.sleep(3)\n",
    "        \n",
    "        after_df = DatabaseConnection.get_table_data('institution')\n",
    "        after_count = len(after_df) if not after_df.empty else 0\n",
    "        print(f\"Rows after insert: {after_count}\")\n",
    "        \n",
    "        if after_count > before_count:\n",
    "            print(f\"Success! Added {after_count - before_count} row(s)\")\n",
    "            \n",
    "            # Try to find our specific record\n",
    "            search_query = f\"\"\"\n",
    "            SELECT * FROM institution \n",
    "            WHERE institution_cpi = '{test_data['institution_cpi']}'\n",
    "            \"\"\"\n",
    "            result = DatabaseConnection.execute_query(search_query)\n",
    "            \n",
    "            if not result.empty:\n",
    "                print(\"Record found in table:\")\n",
    "                print(result.to_string(index=False))\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Record not found when searching specifically\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"Row count did not increase\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Insert operation failed\")\n",
    "        return False\n",
    "\n",
    "def test_bulk_insert():\n",
    "    \"\"\"Test bulk insert of multiple records\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING BULK INSERT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    test_data_list = [\n",
    "        {\n",
    "            'institution_cpi': f'Bulk Test Institution 1 {timestamp}',\n",
    "            'institution_type_layer1': 'Public',\n",
    "            'institution_type_layer2': 'Government',\n",
    "            'country_sub': 'Canada',\n",
    "            'country_parent': 'Canada',\n",
    "            'last_verified': datetime.now().year\n",
    "        },\n",
    "        {\n",
    "            'institution_cpi': f'Bulk Test Institution 2 {timestamp}',\n",
    "            'institution_type_layer1': 'Private',\n",
    "            'institution_type_layer2': 'Funds',\n",
    "            'institution_type_layer3': 'Venture Capital Fund',\n",
    "            'country_sub': 'United Kingdom',\n",
    "            'country_parent': 'United Kingdom',\n",
    "            'last_verified': datetime.now().year\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"Inserting {len(test_data_list)} records\")\n",
    "    \n",
    "    # Get row count before\n",
    "    before_df = DatabaseConnection.get_table_data('institution')\n",
    "    before_count = len(before_df) if not before_df.empty else 0\n",
    "    print(f\"Rows before bulk insert: {before_count}\")\n",
    "    \n",
    "    # Attempt bulk insert\n",
    "    success = DatabaseConnection.bulk_insert('institution', test_data_list)\n",
    "    \n",
    "    if success:\n",
    "        print(\"Bulk insert operation completed successfully\")\n",
    "        \n",
    "        # Check row count after\n",
    "        print(\"Waiting 3 seconds for consistency...\")\n",
    "        import time\n",
    "        time.sleep(3)\n",
    "        \n",
    "        after_df = DatabaseConnection.get_table_data('institution')\n",
    "        after_count = len(after_df) if not after_df.empty else 0\n",
    "        print(f\"Rows after bulk insert: {after_count}\")\n",
    "        \n",
    "        if after_count >= before_count + len(test_data_list):\n",
    "            print(f\"Success! Added {after_count - before_count} row(s)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Expected row count increase not found\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Bulk insert operation failed\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    print(\"TESTING SINGLE-FILE TABLE INSERTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check basic structure\n",
    "    structure_ok = check_table_structure()\n",
    "    if not structure_ok:\n",
    "        print(\"❌ Table structure check failed\")\n",
    "        return\n",
    "    \n",
    "    # Read current data\n",
    "    current_data = read_current_data()\n",
    "    if current_data is None:\n",
    "        print(\"❌ Could not read current data\")\n",
    "        return\n",
    "    \n",
    "    # Test single insert\n",
    "    single_success = test_single_insert()\n",
    "    \n",
    "    # Test bulk insert\n",
    "    bulk_success = test_bulk_insert()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if single_success and bulk_success:\n",
    "        print(\"🎉 ALL TESTS PASSED!\")\n",
    "        print(\"Your insert logic is now working correctly.\")\n",
    "    elif single_success:\n",
    "        print(\"✅ Single insert works, ❌ bulk insert failed\")\n",
    "    elif bulk_success:\n",
    "        print(\"❌ Single insert failed, ✅ bulk insert works\")\n",
    "    else:\n",
    "        print(\"❌ Both tests failed\")\n",
    "        print(\"Check the error messages above for debugging\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232bc202-8349-449f-a4aa-b14a9275bfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
